{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from ripser import Rips\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import ruptures as rpt\n",
    "import gudhi as gd\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in jumps in a Voronoi cell, along with a threshold for what a dominant jump is, and calculates\n",
    "#relevant statistics and the predicted period, if possible\n",
    "class Jump_Summary:\n",
    "    def __init__(self, jumps, epsilon):\n",
    "        self.jumps = jumps\n",
    "        self.epsilon = epsilon\n",
    "        self.small_jump_totals = {i : [] for i in range(len(jumps))}\n",
    "        self.large_jumps = {i : [] for i in range(len(jumps))}\n",
    "        self.small_jump_totals_variance = []\n",
    "        self.large_jumps_average = []\n",
    "        self.period_average = 0\n",
    "        self.period_variance = 0\n",
    "        \n",
    "        for landmark in range(len(jumps)):\n",
    "            i = 0\n",
    "            jump_lst = jumps[landmark]\n",
    "            for jump in jump_lst:\n",
    "                if jump < epsilon:\n",
    "                    i += 1\n",
    "                else:\n",
    "                    self.large_jumps[landmark].append(jump)\n",
    "                    self.small_jump_totals[landmark].append(i)\n",
    "                    i = 0\n",
    "            if len(self.large_jumps[landmark]) == 0:\n",
    "                continue\n",
    "            try:\n",
    "                avg = sum(self.small_jump_totals[landmark]) / len(self.small_jump_totals[landmark])\n",
    "                self.small_jump_totals_variance.append(sum((x-avg)**2 for x in self.small_jump_totals[landmark]) / len(self.small_jump_totals[landmark]))\n",
    "            except:\n",
    "                print(\"No small jumps for Landmark \", landmark)\n",
    "            self.large_jumps_average.append(sum(self.large_jumps[landmark]) / len(self.large_jumps[landmark]))\n",
    "        try:\n",
    "            self.period_average = sum(self.large_jumps_average) / len(self.large_jumps_average)\n",
    "            self.period_variance = sum((x-self.period_average)**2 for x in self.large_jumps_average) / len(self.large_jumps_average)\n",
    "        except:\n",
    "            print(\"Not enough data to get period estimate, returning 0\")\n",
    "            self.period_average = 0\n",
    "            self.period_variance = 0\n",
    "    \n",
    "    def print_summary(self):\n",
    "        print(\"Epsilon: \", self.epsilon)\n",
    "        print(\"Number of Landmarks: \", len(self.jumps))\n",
    "        print(\"\")\n",
    "        for i in range(len(self.jumps)):\n",
    "            try:\n",
    "                avg = sum(self.small_jump_totals[i]) / len(self.small_jump_totals[i])\n",
    "                lavg = sum(self.large_jumps[i]) / len(self.large_jumps[i])\n",
    "                print(\"Landmark \", i, \" Small Jump Totals: \", self.small_jump_totals[i])\n",
    "                print(\"Landmark \", i, \" Small Jump Totals Average: \", avg)\n",
    "                print(\"Landmark \", i, \" Small Jump Totals Variance: \", \n",
    "                    sum((x-avg)**2 for x in self.small_jump_totals[i]) / len(self.small_jump_totals[i]))\n",
    "                print(\"Landmark \", i, \" Large Jumps: \", self.large_jumps[i])\n",
    "                print(\"Landmark \", i, \" Large Jumps Average: \", lavg)\n",
    "                print(\"Landmark \", i, \" Large Jump Variance: \", sum((x-lavg)**2 for x in self.large_jumps[i]) / len(self.large_jumps[i]))\n",
    "                print(\"\")\n",
    "            except:\n",
    "                print(\"Not enough data for Landmark \", i)\n",
    "                print(\"\")\n",
    "        print(\"Average Period (in steps): \", self.period_average)\n",
    "        print(\"Period (in steps) Variance: \", self.period_variance)\n",
    "\n",
    "#Turns a time series into a specific subset of the data\n",
    "def get_window_from_series(time_series, m, end):\n",
    "    window = time_series[max([0,end-m]):end+1]\n",
    "    return window\n",
    "\n",
    "#Turns a time series into a collection of windows  to slide along\n",
    "def sliding_window_embedding(time_series, n, d):\n",
    "    size = len(time_series)\n",
    "    swe = [0 for x in range(size - n*d)]\n",
    "    for i in range(len(swe)):\n",
    "        swe[i] = [time_series[i + k*d] for k in range(n+1)]\n",
    "    return np.array(swe)\n",
    "\n",
    "#If possible, makes a persistence diagram from the data\n",
    "def plot_persistent_homology_diagram(data, plot=True):\n",
    "    try:\n",
    "        rips = Rips(verbose = False)\n",
    "        diagrams = rips.fit_transform(data)\n",
    "        if plot:\n",
    "            rips.plot(diagrams)\n",
    "        return diagrams\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "#Calculates the l1 and l2 norms of the persistence diagram\n",
    "def calculate_norms(diagram, dimension=1):\n",
    "    l1 = 0\n",
    "    l2 = 0\n",
    "    for entry in diagram[dimension]:\n",
    "        l1 = l1 + abs(entry[0]) + abs(entry[1])\n",
    "        l2 = l2 + entry[0]*entry[0]+entry[1]*entry[1]\n",
    "    l2 = math.sqrt(l2)\n",
    "    return[l1,l2]\n",
    "\n",
    "#Selects n equally spaced apart points to be center points in the swe and assigns each point  in the SWE\n",
    "#to a corresponding Voronoi cell\n",
    "def generate_voronoi_cells(swe, num_cells):\n",
    "    kmeans = KMeans(n_clusters=num_cells).fit(swe)\n",
    "    landmarks = kmeans.cluster_centers_\n",
    "    cells = {i : [] for i in range(len(landmarks))}\n",
    "    time_index = 0\n",
    "    for point in swe:\n",
    "        distances = [math.dist(point,landmark) for landmark in landmarks]\n",
    "        cell = np.argmin(distances)\n",
    "        cells[cell].append(time_index)\n",
    "        time_index += 1\n",
    "    return landmarks, cells\n",
    "\n",
    "#Gets the distance between time series indicies in a Voronoi cell\n",
    "def generate_vector_jumps(landmarks,cells):\n",
    "    j = {i : [] for i in range(len(landmarks))}\n",
    "    for landmark in range(len(landmarks)):\n",
    "        t = cells[landmark]\n",
    "        for index in range(1,len(t)):\n",
    "            j[landmark].append(t[index]-t[index-1])\n",
    "    return j\n",
    "\n",
    "#Converts the estimated period into the proper time unit\n",
    "def estimate_period(summary, time_step):\n",
    "    return summary.period_average * time_step\n",
    "\n",
    "#DEPRICATED\n",
    "def voronoi_estimations(swe, epsilon, time_step, min_cells, max_cells):\n",
    "    voronoi = []\n",
    "    periods = []\n",
    "    for cell_num in range(min_cells,max_cells+1):\n",
    "        print(\"Vornoi Cell Number Amount: \", cell_num)\n",
    "        landmarks, cells = generate_voronoi_cells(swe, cell_num)\n",
    "        jumps = generate_vector_jumps(landmarks,cells)\n",
    "        summary = Jump_Summary(jumps, epsilon)\n",
    "        if summary.valid_summary:\n",
    "            periods.append(estimate_period(summary, time_step))\n",
    "            voronoi.append(cell_num)\n",
    "    return voronoi, periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = pd.read_csv(r\"~/TDA/Data/water_full.csv\")\n",
    "time_df = pd.read_csv(r\"~/TDA/Data/water_data_qfneg.csv\")\n",
    "predicted_df.insert(0,'TIME',time_df['TIME'],True)\n",
    "#predicted_df = predicted_df[predicted_df['FLDNUM']=='Havana, IL']\n",
    "predicted_df[\"MONTH\"] = pd.DatetimeIndex(predicted_df[\"DATE\"]).month\n",
    "predicted_df[\"DATE\"] = pd.to_datetime(predicted_df[\"DATE\"])\n",
    "predicted_df[\"SEASON\"] = predicted_df[\"MONTH\"]\n",
    "seasons = {3 : 'SPRING',\n",
    "           4 : 'SPRING',\n",
    "           5 : 'SPRING',\n",
    "           6 : 'SUMMER',\n",
    "           7 : 'SUMMER',\n",
    "           8 : 'SUMMER',\n",
    "           9 : 'FALL',\n",
    "           10 : 'FALL',\n",
    "           11: 'FALL',\n",
    "           12: 'WINTER',\n",
    "           1: 'WINTER',\n",
    "           2: 'WINTER'}\n",
    "predicted_df = predicted_df.replace({\"SEASON\" : seasons})\n",
    "\n",
    "stratum = {'Main channel' : 1, 'Side channel' : 2, 'Backwater area contiguous to the main channel' : 3}\n",
    "predicted_df = predicted_df.replace({\"STRATUM\" : stratum})\n",
    "predicted_df.sort_values(by=[\"DATE\",\"TIME\"], inplace=True)\n",
    "\n",
    "#IDEA: Make a new column called Modified Time. Set earliest sample to 0. Let all other samples be days\n",
    "#since first sample (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "for index, row in predicted_df.iterrows():\n",
    "    time = row[\"TIME\"]\n",
    "    time = time.split(\":\")\n",
    "    h = int(time[0])\n",
    "    m = int(time[1])\n",
    "    date.append(row[\"DATE\"] + pd.DateOffset(hours=h, minutes=m))\n",
    "date = np.array(date)\n",
    "predicted_df[\"DATE\"] = date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993   SPRING   0\n",
      "1993   SUMMER   388\n",
      "1993   FALL   504\n",
      "1993   WINTER   0\n",
      "1994   SPRING   627\n",
      "1994   SUMMER   759\n",
      "1994   FALL   108\n",
      "1994   WINTER   584\n",
      "1995   SPRING   734\n",
      "1995   SUMMER   558\n",
      "1995   FALL   722\n",
      "1995   WINTER   410\n",
      "1996   SPRING   735\n",
      "1996   SUMMER   767\n",
      "1996   FALL   734\n",
      "1996   WINTER   0\n",
      "1997   SPRING   776\n",
      "1997   SUMMER   741\n",
      "1997   FALL   745\n",
      "1997   WINTER   707\n",
      "1998   SPRING   746\n",
      "1998   SUMMER   782\n",
      "1998   FALL   742\n",
      "1998   WINTER   786\n",
      "1999   SPRING   773\n",
      "1999   SUMMER   743\n",
      "1999   FALL   712\n",
      "1999   WINTER   659\n",
      "2000   SPRING   798\n",
      "2000   SUMMER   811\n",
      "2000   FALL   739\n",
      "2000   WINTER   540\n",
      "2001   SPRING   820\n",
      "2001   SUMMER   799\n",
      "2001   FALL   793\n",
      "2001   WINTER   665\n",
      "2002   SPRING   782\n",
      "2002   SUMMER   764\n",
      "2002   FALL   0\n",
      "2002   WINTER   754\n",
      "2003   SPRING   0\n",
      "2003   SUMMER   0\n",
      "2003   FALL   0\n",
      "2003   WINTER   0\n",
      "2004   SPRING   823\n",
      "2004   SUMMER   800\n",
      "2004   FALL   765\n",
      "2004   WINTER   589\n",
      "2005   SPRING   836\n",
      "2005   SUMMER   773\n",
      "2005   FALL   801\n",
      "2005   WINTER   768\n",
      "2006   SPRING   816\n",
      "2006   SUMMER   756\n",
      "2006   FALL   784\n",
      "2006   WINTER   781\n",
      "2007   SPRING   815\n",
      "2007   SUMMER   740\n",
      "2007   FALL   826\n",
      "2007   WINTER   753\n",
      "2008   SPRING   837\n",
      "2008   SUMMER   823\n",
      "2008   FALL   753\n",
      "2008   WINTER   720\n",
      "2009   SPRING   827\n",
      "2009   SUMMER   789\n",
      "2009   FALL   745\n",
      "2009   WINTER   625\n",
      "2010   SPRING   840\n",
      "2010   SUMMER   802\n",
      "2010   FALL   839\n",
      "2010   WINTER   746\n",
      "2011   SPRING   831\n",
      "2011   SUMMER   810\n",
      "2011   FALL   801\n",
      "2011   WINTER   688\n"
     ]
    }
   ],
   "source": [
    "years = [x for x in range(1993,2012)]\n",
    "seasons = [\"SPRING\", \"SUMMER\", \"FALL\", \"WINTER\"]\n",
    "for year in years:\n",
    "    for season in seasons:\n",
    "        test = predicted_df[predicted_df['YEAR']==year]\n",
    "        test = test[test['SEASON']==season]\n",
    "        print(year, \" \", season, \" \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 60\n",
    "years = [x for x in range(1993,2002)]\n",
    "seasons = [\"SPRING\", \"SUMMER\", \"FALL\", \"WINTER\"]\n",
    "for year in years:\n",
    "    for season in seasons:\n",
    "        test = predicted_df[predicted_df['YEAR']==year]\n",
    "        test = test[test['SEASON']==season]\n",
    "        if(len(test) < n):\n",
    "            print(\"Year \", year, \" season \", season, \" has \", len(test), \" points!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {x : [] for x in predicted_df.columns}\n",
    "fix_seasons = pd.DataFrame(columns)\n",
    "for year in years:\n",
    "    print(year)\n",
    "    year_df = predicted_df[predicted_df[\"YEAR\"]==year]\n",
    "    for season in seasons:\n",
    "        seasonal_df = year_df[year_df[\"SEASON\"]==season]\n",
    "        counter = 0\n",
    "        for index, row in seasonal_df.iterrows():\n",
    "            if counter >= n:\n",
    "                break\n",
    "            fix_seasons.loc[len(fix_seasons.index)] = row\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seasons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#will use a window of size 600\n",
    "time_series = np.array(fix_seasons[\"TEMP\"])\n",
    "time_step = 1.0/240\n",
    "periods = []\n",
    "#for i in range(len(time_series)):\n",
    "for i in range(1001):\n",
    "    print(\"i: \", i)\n",
    "    window = get_window_from_series(time_series, 600, i)\n",
    "    swe = sliding_window_embedding(window, 30, 6)\n",
    "    diagram = plot_persistent_homology_diagram(swe, False)\n",
    "    if diagram == 0:\n",
    "        periods.append(0)\n",
    "        print(\"Not enough data to do SWE, returning 0\")\n",
    "        continue\n",
    "    l1, l2 = calculate_norms(diagram)\n",
    "    try:\n",
    "        landmarks, cells = generate_voronoi_cells(swe, 30)\n",
    "    except ValueError:\n",
    "        periods.append(0)\n",
    "        print(\"Failed to make Voronoi cells, returning 0\")\n",
    "        continue\n",
    "    jumps = generate_vector_jumps(landmarks,cells)\n",
    "    summary = Jump_Summary(jumps, 20)\n",
    "    periods.append(estimate_period(summary, time_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=80\n",
    "plt.plot(periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swe = sliding_window_embedding(time_series, 30, 8)\n",
    "diagram = plot_persistent_homology_diagram(swe, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2 = calculate_norms(diagram)\n",
    "print(l1)\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks, cells = generate_voronoi_cells(swe, 30)\n",
    "jumps = generate_vector_jumps(landmarks,cells)\n",
    "summary = Jump_Summary(jumps, 20)\n",
    "summary.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = estimate_period(summary, time_step)\n",
    "#print(\"Real Period: \", real_period)\n",
    "print(\"Period Esitmation: \", period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fix_seasons[\"TP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will use a window of size 600\n",
    "time_series = np.array(fix_seasons[\"TN\"])\n",
    "time_step = 1.0/240\n",
    "periods = []\n",
    "#for i in range(len(time_series)):\n",
    "for i in range(1001):\n",
    "    print(\"i: \", i)\n",
    "    window = get_window_from_series(time_series, 600, i)\n",
    "    swe = sliding_window_embedding(window, 30, 6)\n",
    "    diagram = plot_persistent_homology_diagram(swe, False)\n",
    "    if diagram == 0:\n",
    "        periods.append(0)\n",
    "        print(\"Not enough data to do SWE, returning 0\")\n",
    "        continue\n",
    "    l1, l2 = calculate_norms(diagram)\n",
    "    print(\"L1: \", l1)\n",
    "    print(\"L2: \", l2)\n",
    "    try:\n",
    "        landmarks, cells = generate_voronoi_cells(swe, 30)\n",
    "    except ValueError:\n",
    "        periods.append(0)\n",
    "        print(\"Failed to make Voronoi cells, returning 0\")\n",
    "        continue\n",
    "    jumps = generate_vector_jumps(landmarks,cells)\n",
    "    summary = Jump_Summary(jumps, 20)\n",
    "    periods.append(estimate_period(summary, time_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(periods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1997, 1998, 1999, 2000, 2001, 2002, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011]\n"
     ]
    }
   ],
   "source": [
    "years = [x for x in range(1997,2003)]\n",
    "years += [x for x in range(2004,2012)]\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31113   2002-10-08 09:01:00\n",
       "31114   2002-10-08 09:14:00\n",
       "31115   2002-10-08 09:19:00\n",
       "31116   2002-10-08 09:31:00\n",
       "31117   2002-10-08 09:37:00\n",
       "                ...        \n",
       "3994    2002-10-19 12:50:00\n",
       "44777   2002-10-19 12:53:00\n",
       "44778   2002-10-19 13:02:00\n",
       "44779   2002-10-19 13:13:00\n",
       "44780   2002-10-19 13:24:00\n",
       "Name: DATE, Length: 793, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impute = predicted_df[predicted_df['YEAR']==2001]\n",
    "impute = impute[impute['SEASON']=='FALL']\n",
    "year = [2002 for x in range(len(impute))]\n",
    "year = np.array(year)\n",
    "impute.YEAR = year\n",
    "lst = []\n",
    "for index, row in impute['DATE'].items():\n",
    "    lst.append(row.replace(year=2002))\n",
    "lst = np.array(lst)\n",
    "impute.DATE = lst\n",
    "predicted_df.append(impute)\n",
    "predicted_df.sort_values(by=[\"DATE\",\"TIME\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
